# AILawMiniCourseHW
##Supreme Court

###Step 1:
Before we can analyze the content of supreme court cases and their relationships with other cases, we need a dataset that containing the text of all cases.
First, the code assembles a list of all supreme court cases within a chosen period (here 1760 to 2018) and collects the URLs for each of these cases from casetext.com.

###Step 2:
Next, the program uses the case names and URLs from step one to pull the text of each case from casetext.com. Requests are split into smaller chunks so that the website does not block the IP address of the computer executing the code. By the end of step 2, the program has assembled a table containing, for each supreme court case, the case’s URL, title, year, and decision text.

###Step 3:
Not all words contained within each supreme court decision carry significance when trying to separate cases by topic. So the program removes words and characters that will not be useful in learning about the relationships between cases and assigning topics to these relationships.  Case names, peoples names, place names and geographical indicators, date indicators such as months, and some law specific words with only procedural significance such as ‘court’ or ‘writ’ are removed from the dataset.
By the end of step 3, our dataset consists of case URLs, titles, years, and ‘cleaned’ text for each case.

###Step 4:
Now that our data are assembled and cleaned, we need to chose which model will be used to group related words within cases together and establish our topics.
The program tests three topic models: LDA, LSA, NMF to try to find which is generates the most useful topic identities. Each of these models is implemented by using the Sci-Kit library, which has functions NMF, LatentDirichletAllocation, and TruncatedSVD.
Applying the 3 models to our dataset, it is clear that the NMF model works best--the topics (clusters of associated words) generated by the LDA and LSA models appear to have less in common than the topics generated by the NMF model.

###Step 5:
Finally, the program applies NMF to the data collected in steps 1-3 to create find clusters of words that are associated with each other.

 
create a dictionary that works in reverse: words (opinions from case text) go in, topics come out
visualizing topics as they appear in cases over the years


##TensorFlow word2vec Example

###Step 1:
First, we need a data set to train our model. The function maybe_download downloads a data file from the web which contains a large set of english sentences. read_data extracts zip file as string.

###Step 2:
Next, the program selects the 50,000 most commonly used the words in the downloaded data file and assigns each word a numeric code. The program creates a dictionary relating each of the most common words to its code, and another dictionary relating these codes back to their associated word.

###Step 3: 
Because it would be computationally expensive to feed the entire data set to our model at once, we need to break it into smaller chunks first.
So the program generates batches of data to feed into the TensorFlow algorithm.

###Step 4:
Next, we select the parameters for our model (how many words in each direction we will look at, how big our small batches will be). We also set up the a validation procedure by randomly picking some of the most common words from the original data set and monitoring which other words the model associates with them.

step 5:
With these parameters set, we train the model defined in step 4 using the small chunks of text that we generated in step 3. The program regularly prints the words closely associated to our randomly chosen validation words so we can see how it is doing. After feeding the model 10,000 chunks of words, we have the final_embeddings which represents which words are relatively 'close' to each other.

step 6:
To visualize which words the model things are similar, the program graphs the 500 most common words based on the TensorFlow results-- 'similar' words (contextually) appear close to each other.




